# 8.10 - ElastiCache Strategies

## Key Concepts

### 1. When to Cache (and When Not To)

| Question | Good for caching | Bad for caching |
|---|---|---|
| **Is it safe to cache?** | Data can be eventually consistent | Data must always be real-time accurate (e.g., bank balance) |
| **Is caching effective?** | Data changes slowly, few keys requested frequently | Data changes rapidly, entire key space needed |
| **Is the data structured well?** | Key-value pairs, aggregation results | Complex relational queries |
| **Which strategy fits?** | Lazy Loading, Write Through, or both | No clear fit |

### 2. Lazy Loading (Cache-Aside / Lazy Population)

All three names mean the same thing.

```
   ┌─────────┐  1. Query cache   ┌─────────────┐
   │         │───────────────────►│ ElastiCache │
   │         │◄───────────────────│             │
   │         │  2a. Cache Hit ✅  └─────────────┘
   │         │
   │   App   │  2b. Cache Miss ❌
   │         │
   │         │  3. Query DB       ┌─────────────┐
   │         │───────────────────►│     RDS     │
   │         │◄───────────────────│             │
   │         │  (returns data)    └─────────────┘
   │         │
   │         │  4. Write to cache ┌─────────────┐
   │         │───────────────────►│ ElastiCache │
   └─────────┘                    └─────────────┘
```

| Pros | Cons |
|---|---|
| Only requested data gets cached (efficient) | Cache miss = **3 network calls** (read penalty) |
| Node failure is not fatal — cache just warms up again | **Stale data** — if RDS is updated, cache still has old value |

**Pseudocode:**
```python
def get_user(user_id):
    record = cache.get(user_id)       # 1. Check cache
    if record is None:                # 2. Cache miss?
        record = db.query(...)        # 3. Read from DB
        cache.set(user_id, record)    # 4. Write to cache
    return record                     # Cache hit or DB result
```

Pattern to recognize: **read function** → check cache → if miss, read DB → write to cache.

### 3. Write Through

Caches data **at write time** — every write to DB also writes to cache.

```
   ┌─────────┐  1. Read from cache ┌─────────────┐
   │         │────────────────────►│ ElastiCache │
   │         │◄────────────────────│             │
   │         │  Cache Hit ✅       └─────────────┘
   │   App   │
   │         │  2. Write to DB     ┌─────────────┐
   │         │────────────────────►│     RDS     │
   │         │                     └─────────────┘
   │         │
   │         │  3. Write to cache  ┌─────────────┐
   │         │────────────────────►│ ElastiCache │
   └─────────┘                     └─────────────┘
```

| Pros | Cons |
|---|---|
| Cache is **never stale** — always in sync with DB | **Write penalty** — every write = 2 calls (DB + cache) |
| Users tolerate writes being slower more than reads | **Missing data** until first write happens (cache starts empty) |
| | **Cache churn** — lots of data cached that may never be read |

**Pseudocode:**
```python
def save_user(user_id, values):
    record = db.query("UPDATE users ...", values)  # 1. Write to DB
    cache.set(user_id, record)                      # 2. Write to cache
    return record
```

Pattern to recognize: **write function** → save to DB → update cache.

**Best practice:** combine Lazy Loading (reads) + Write Through (writes).

### 4. Cache Eviction & TTL

| Method | How it works |
|---|---|
| **Explicit delete** | App manually removes an item from the cache |
| **LRU (Least Recently Used)** | Cache full → evicts least recently accessed item |
| **TTL (Time-to-Live)** | Set expiration time → item auto-evicts when it expires |

- TTL can range from **seconds** to **hours/days**
- Even a few seconds of TTL can be hugely effective for hot data
- Great for: leaderboards, comments, activity streams, user profiles
- Too many evictions → **scale up your cache**
- TTL is unnecessary with Write Through (data is already kept fresh on writes)

### Strategy Priority

| Strategy | Use case | Priority |
|---|---|---|
| **Lazy Loading** | Foundation for most apps, improves reads | Implement first |
| **Write Through** | Add on top of Lazy Loading to reduce staleness | Implement second, if needed |
| **TTL** | Balance between freshness and cache size | Good default, but unnecessary with Write Through |

---

## Exam Tips
- Know the three names: **Lazy Loading = Cache-Aside = Lazy Population**
- Be able to **read pseudocode** and identify which strategy it is (read function = Lazy Loading, write function = Write Through)
- Lazy Loading: read penalty (3 calls on miss), stale data possible, node failure = warm up
- Write Through: write penalty (2 calls on write), never stale, cache starts empty
- Combine both for best results
- TTL: great default, range from seconds to days
- Too many LRU evictions → scale up cache
- Cache only data that makes sense (user profiles, blogs — NOT bank balances, pricing)
